{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f285e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 15 nodes and 24 edges\n",
      "Average Clustering Coefficient: 0.2977777777777778\n",
      "Average Degree: 3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suman\\AppData\\Local\\Temp\\ipykernel_18336\\2499503137.py:29: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  df = pd.read_csv('CVoteRank.csv', squeeze=True)\n"
     ]
    }
   ],
   "source": [
    "#Read the network\n",
    "import networkx as nx\n",
    "from networkx.algorithms.centrality import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import random\n",
    "from math import *\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import community.community_louvain\n",
    "\n",
    "\n",
    "####################################### Set path to upload file ##############################################\n",
    "G=nx.Graph()\n",
    "os.chdir(r\"C:\\Users\\suman\\CVoteRank\") #Network Dataset -Community Based\\K-Air traffic control\n",
    "file_name='CVoteRank'\n",
    "#read a graph in text/CSV format\n",
    "#G = nx.read_edgelist(\"hi1.txt\", create_using = nx.Graph(), nodetype=int)\n",
    "\n",
    "\n",
    "####################################### Upload graph as a .CSV file ##########################################\n",
    "G = nx.Graph()\n",
    "df = pd.read_csv('CVoteRank.csv', squeeze=True)\n",
    "G = nx.Graph()\n",
    "nod = []\n",
    "for i in range(df.shape[0] - 0):\n",
    "    nod.append(df['Node1'][i])\n",
    "    nod.append(df['Node2'][i])\n",
    "tmp = set(nod)\n",
    "nod = list(tmp)\n",
    "G.add_nodes_from(nod)\n",
    "for i in range(df.shape[0] - 0):\n",
    "    #G.add_edge(df['Node1'][i], df['Node2'][i], weight=df['Weight'][i])\n",
    "    G.add_edge(df['Node1'][i], df['Node2'][i])\n",
    "    \n",
    "g=G\n",
    "print(G)\n",
    "# Average Clustering Coefficient\n",
    "avg_clustering = nx.average_clustering(G)\n",
    "print(f\"Average Clustering Coefficient: {avg_clustering}\")\n",
    "\n",
    "# Average Degree\n",
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "avg_degree = 2 * num_edges / num_nodes\n",
    "print(f\"Average Degree: {avg_degree}\")    \n",
    "    \n",
    "\n",
    "############################################## Organize Community ############################################\n",
    "\n",
    "def flip_nodes_and_communities(dict_nodes_communities):\n",
    "    # Step 1: initialize communities as keys\n",
    "    new_dict = {}\n",
    "    for k, v in dict_nodes_communities.items():\n",
    "        new_dict[v]=[]\n",
    "    \n",
    "    # Step 2: Fill in nodes\n",
    "    for kk,vv in new_dict.items():\n",
    "        for k,v in dict_nodes_communities.items():\n",
    "            if dict_nodes_communities[k] == kk:\n",
    "                new_dict[kk].append(k)\n",
    "    \n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def orderCommunities(c):\n",
    "    # We index each community starting from 0 and put them in list keys_partition\n",
    "    # So, if we have 11 communities, then the indexing list keys_partition = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "    # Use `j` to move till the length of c and iterate `i` = i+1 to organize the community numbering\n",
    "    # say it is #20, as index 0 and the second community, say #49 as index 1, etc...\n",
    "    i = 0\n",
    "    keys_partition = list()\n",
    "    for j in c:\n",
    "        keys_partition.append(i)\n",
    "        i = i + 1\n",
    "\n",
    "    # Then for each index i aka for each community, we will create a dictionary partition where i is the key\n",
    "    partition = dict()\n",
    "    for i in keys_partition:\n",
    "        partition[i] = []\n",
    "\n",
    "    # Now we have to fill the partition dictionary\n",
    "    i = 0\n",
    "    for j in c:  # for each community j in c\n",
    "        for k in c[j]:  # for each node in community c\n",
    "            partition[i].append(k)  # for partition i (organized), append the nodes in this community\n",
    "        i = i + 1\n",
    "\n",
    "    return partition\n",
    "\n",
    "def communityInfo(c, partition):\n",
    "    print('Number of partitions: ', len(partition))\n",
    "    l = list()\n",
    "    for i in c:  # for all communities i in c\n",
    "        for j in c[i]:  # for all nodes in communities c[i]\n",
    "            l.append(j)  # add the nodes to l\n",
    "\n",
    "    print('Number of nodes in the communities detected: ', len(l))\n",
    "\n",
    "    s = set(l)  # removes repetitive nodes since set() involves only unique inputs\n",
    "    print('Number of repetitions: ', len(l) - len(s))\n",
    "\n",
    "\n",
    "def dict_communities_organized_G(G):\n",
    "    partition = community.community_louvain.best_partition(G)\n",
    "    communities=partition\n",
    "    #print(\"modularity is\",community.community_louvain.modularity(partition, G))\n",
    "    # Step 1\n",
    "    dict_communities = flip_nodes_and_communities(communities)\n",
    "    #print(\"infomap_communities\",infomap_communities)\n",
    "    #dictionary of keys as community number and values as nodes in that community\n",
    "    \n",
    "    # Step 2\n",
    "    dict_communities_organized = orderCommunities(dict_communities)\n",
    "    #print(\"infomap_communities_organized\",infomap_communities_organized)\n",
    "    #Arrange in decending order of infomap_communities\n",
    "    \n",
    "    #communityInfo(dict_communities, dict_communities_organized)\n",
    "    return partition, dict_communities_organized\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "#graph extractor\n",
    "def isbelong(list1, list2):\n",
    "    x = 0\n",
    "    for i in list1:\n",
    "        for j in list2:\n",
    "            if i == j:\n",
    "                x = 1\n",
    "    return x\n",
    "\n",
    "def isexist(x, list1):\n",
    "    k = 0\n",
    "    for i in list1:\n",
    "        if x == i:\n",
    "            k = 1\n",
    "    return k\n",
    "\n",
    "def intra_wo(dict_graph_wo, partition):\n",
    "    dict_node_partition_wo = dict()\n",
    "    for i in dict_graph_wo: \n",
    "        dict_node_partition_wo[i] = [] \n",
    "        for j in partition:  \n",
    "            for k in partition[j]:\n",
    "                if i == k:\n",
    "                    dict_node_partition_wo[i].append(j)\n",
    "\n",
    "    d_copy = dict_graph_wo.copy()\n",
    "    dict_g_intra_wo = dict()\n",
    "    for i in d_copy:\n",
    "        dict_g_intra_wo[i] = []\n",
    "        for j in d_copy[i]:\n",
    "            k = isbelong(dict_node_partition_wo[i], dict_node_partition_wo[j])  \n",
    "            if k == 1:\n",
    "                dict_g_intra_wo[i].append(j)\n",
    "\n",
    "    graph_intra_wo = nx.Graph()\n",
    "    for i in dict_g_intra_wo:\n",
    "        graph_intra_wo.add_node(i)\n",
    "\n",
    "    for i in dict_g_intra_wo:\n",
    "        for j in dict_g_intra_wo[i]:\n",
    "            graph_intra_wo.add_edges_from([(i, j)])\n",
    "\n",
    "    return dict_g_intra_wo, graph_intra_wo\n",
    "\n",
    "\n",
    "def intra_o(g, partition, lo):\n",
    "\n",
    "    dict_node_partition = dict ()\n",
    "    for i in g:  \n",
    "        dict_node_partition[i] = [] \n",
    "        for j in partition: \n",
    "            for k in partition[j]:  \n",
    "                if i == k:  \n",
    "                    dict_node_partition[i].append(j)\n",
    "\n",
    "\n",
    "    s = set()\n",
    "    for i in lo:\n",
    "        for j in dict_node_partition[i]:\n",
    "            s.add(j) \n",
    "    s_sup = set()\n",
    "    for i in partition:\n",
    "        s_sup.add(i)  \n",
    "\n",
    "    for i in s: \n",
    "        s_sup.remove(i) \n",
    "\n",
    " \n",
    "    g_copy = g.copy()\n",
    "    for i in s_sup:\n",
    "        for j in partition[i]:\n",
    "            g_copy.remove_node(j)\n",
    "\n",
    "\n",
    "    dict_g_intra_o = dict()\n",
    "    for i in g_copy:\n",
    "        dict_g_intra_o[i] =[]\n",
    "        for j in g[i]:\n",
    "            k = isbelong(dict_node_partition[i], dict_node_partition[j])  # Do they share any community? If so, append\n",
    "            if k == 1:\n",
    "                dict_g_intra_o[i].append(j)\n",
    "\n",
    "\n",
    "    graph_intra_o = nx.Graph()\n",
    "    for i in dict_g_intra_o:\n",
    "        graph_intra_o.add_node(i)\n",
    "\n",
    "    for i in dict_g_intra_o:\n",
    "        for j in dict_g_intra_o[i]:\n",
    "            graph_intra_o.add_edges_from([(i, j)])\n",
    "\n",
    "    return dict_g_intra_o, graph_intra_o\n",
    "\n",
    "\n",
    "def inter_wo_o(g, partition):\n",
    "\n",
    "    dict_node_partition = dict()\n",
    "    for i in g:\n",
    "        dict_node_partition[i] = []\n",
    "        for j in partition:\n",
    "            for k in partition[j]:\n",
    "                if i == k:\n",
    "                    dict_node_partition[i].append(j)\n",
    "\n",
    "    dict_g_inter = dict()\n",
    "    for i in g:\n",
    "        dict_g_inter[i] = []\n",
    "        for j in g[i]:\n",
    "            k = isbelong(dict_node_partition[i], dict_node_partition[j])\n",
    "            if k == 0:\n",
    "                dict_g_inter[i].append(j)\n",
    "\n",
    "\n",
    "    graph_inter = nx.Graph()\n",
    "    for i in dict_g_inter:\n",
    "        graph_inter.add_node(i)\n",
    "\n",
    "    for i in dict_g_inter:\n",
    "        for j in dict_g_inter[i]:\n",
    "            graph_inter.add_edges_from([(i, j)])\n",
    "\n",
    "    return dict_g_inter, graph_inter\n",
    "\n",
    "##############################################################################################################\n",
    "#method overlapping\n",
    "\n",
    "def isbelong(l1,l2): # Checks if an item in list 1 exists in list 2\n",
    "    x=0\n",
    "    for i in l1:\n",
    "        for j in l2:\n",
    "            if i==j:\n",
    "                x=1\n",
    "\n",
    "    return x\n",
    "\n",
    "def list_overlapping(g,communities_flipped):\n",
    "    l_nodes = g.nodes()\n",
    "    dict_membership_of_all_nodes=dict() # Prepare the dictionary to contain the membership of all nodes\n",
    "    for i in l_nodes: # For each node\n",
    "        dict_membership_of_all_nodes[i]=0\n",
    "        for j in communities_flipped: # For each community x \n",
    "            for k in communities_flipped[j]: # For each node in that community x\n",
    "                if i==k: # If the node i (starting node) occurs in community x, increment\n",
    "                    dict_membership_of_all_nodes[i]=dict_membership_of_all_nodes[i]+1\n",
    "                    \n",
    "    list_overlapping_nodes=[] # Prepare the list to contain the nodes which only overlap\n",
    "    for i in dict_membership_of_all_nodes: # For each node\n",
    "        if dict_membership_of_all_nodes[i]>1: # If the membership is greater than 1, it is an overlapping node\n",
    "            list_overlapping_nodes.append(i)\n",
    "\n",
    "\n",
    "            \n",
    "    dict_membership_of_overlapping_nodes=dict() # Prepare the dictionary to contain the membership of overlapping nodes only\n",
    "    for i in list_overlapping_nodes: # For each overlapping node\n",
    "        dict_membership_of_overlapping_nodes[i]=dict_membership_of_all_nodes[i] # Get its membership and put it in the dictionary\n",
    "\n",
    "\n",
    "    return list_overlapping_nodes, dict_membership_of_all_nodes, dict_membership_of_overlapping_nodes\n",
    "    \n",
    "    \n",
    "def remove_overlapping(g,list_overlapping_nodes,communities_flipped):\n",
    "\n",
    "    # Copy the dictionary communities_flipped\n",
    "    communities_flipped_copy=dict()\n",
    "    for i in communities_flipped:\n",
    "        communities_flipped_copy[i]=[]\n",
    "    for i in communities_flipped:\n",
    "        for j in communities_flipped[i]:\n",
    "            communities_flipped_copy[i].append (j)\n",
    "     \n",
    "\n",
    "    # Get the nodes and their neighbors in a dictionary `dict_graph`\n",
    "    l_nodes = g.nodes()\n",
    "    dict_graph = dict()  \n",
    "    for i in l_nodes:\n",
    "        dict_graph[i] = [] # nodes in the key and their neighbors in a list\n",
    "    for i in l_nodes: # For each node\n",
    "        iteri = g.neighbors(i) # Get the neighbors\n",
    "        for j in iteri: # Access the neighbors\n",
    "            dict_graph[i].append(j) # Append them to the list \n",
    "            \n",
    "        \n",
    "    # Remove the overlapping nodes from dict_graph\n",
    "    for overlapping_node in list_overlapping_nodes: # For each overlapping node\n",
    "        for i in dict_graph: # For each node (overlapping or not)\n",
    "            for j in dict_graph[i]: # For the neighbor of each node\n",
    "                if j == overlapping_node: # If the neighbor is an overlapping node\n",
    "                    dict_graph[i].remove(overlapping_node) # Remove it\n",
    "        dict_graph.pop(overlapping_node) # Remove overlapping node from list when done\n",
    "\n",
    "    # Remove the overlapping nodes from communities_flipped_copy\n",
    "    for overlapping_node in list_overlapping_nodes: # For each overlapping node\n",
    "        for i in communities_flipped_copy: # For each community\n",
    "            for j in communities_flipped_copy[i]: # For each node in that community\n",
    "                if j == overlapping_node: # If the node is an overalapping node, remove it\n",
    "                    communities_flipped_copy[i].remove(overlapping_node)\n",
    "\n",
    "    # Construct the graph without overlapping nodes\n",
    "    g_without_ov = nx.Graph()\n",
    "    for i in dict_graph:\n",
    "        g_without_ov.add_node(i) # Add the nodes first\n",
    "    l_edges = []\n",
    "    for i in dict_graph: # For each node that doesn't overlap\n",
    "        for j in dict_graph[i]: # For the neighbors\n",
    "            l_edges.append ((i, j)) # Create a list where they connect\n",
    "    g_without_ov.add_edges_from(l_edges)\n",
    "\n",
    "    return g_without_ov,dict_graph,communities_flipped_copy\n",
    "    \n",
    "    \n",
    "def intra_wo(dict_graph_wo,communities_flipped):\n",
    "\n",
    "    dict_node_communities_flipped = dict() \n",
    "    for i in dict_graph_wo: # Get the non-overlapping nodes and the communities they participate in \n",
    "        dict_node_communities_flipped[i] = []\n",
    "        for j in communities_flipped:\n",
    "            for k in communities_flipped[j]:\n",
    "                if i == k:\n",
    "                    dict_node_communities_flipped[i].append(j)\n",
    "                    \n",
    "    d_copy=dict_graph_wo.copy() \n",
    "    dict_graph_intra_wo=dict() # Get the non-overlapping node and its neighbors which are non overlapping in `dict_graph_intra_wo`\n",
    "    for i in d_copy:\n",
    "        dict_graph_intra_wo[i] =[]\n",
    "        for j in d_copy[i]:\n",
    "            k=isbelong(dict_node_communities_flipped[i],dict_node_communities_flipped[j])\n",
    "            if k==1:\n",
    "                dict_graph_intra_wo[i].append(j)\n",
    "                \n",
    "    # Construct dict_graph_intra_wo into graph_intra_wo\n",
    "    graph_intra_wo = nx.Graph ()\n",
    "    for i in dict_graph_intra_wo:\n",
    "        graph_intra_wo.add_node (i)\n",
    "\n",
    "    for i in dict_graph_intra_wo:\n",
    "        for j in dict_graph_intra_wo[i]:\n",
    "            graph_intra_wo.add_edges_from ([(i, j)])\n",
    "    \n",
    "    return dict_graph_intra_wo, graph_intra_wo\n",
    "    \n",
    "def intra_o(g,communities_flipped,list_overlapping_nodes):\n",
    "\n",
    "    dict_node_communities_flipped = dict () \n",
    "    for i in g: # Get all the nodes and the communities they participate in \n",
    "        dict_node_communities_flipped[i] = []\n",
    "        for j in communities_flipped:\n",
    "            for k in communities_flipped[j]:\n",
    "                if i == k:\n",
    "                    dict_node_communities_flipped[i].append(j)\n",
    "    g_copy=g.copy()\n",
    "    dict_graph_intra_o=dict()\n",
    "\n",
    "    # Find the list of communities to delete\n",
    "    #print(list_overlapping_nodes)\n",
    "    #print(dict_node_communities_flipped)\n",
    "\n",
    "    c=set() # Will contain the communities of the overlapping nodes\n",
    "    for i in list_overlapping_nodes: # For each overlapping node\n",
    "        for j in dict_node_communities_flipped[i]: # For each community of the overlapping node\n",
    "            c.add(j) # Add the community\n",
    "            \n",
    "    c_sup=set() # Will contain all of the communities to be later processed to contain only communities with non-overlapps in it\n",
    "    for i in communities_flipped: # For each community\n",
    "        c_sup.add(i) # Add the community\n",
    "\n",
    "    for i in c: # Remove communities which have overlaps in them\n",
    "        c_sup.remove(i)\n",
    "        \n",
    "    #Remove the nodes in the communities with no overlaps --> We want overlapping intra nodes only\n",
    "    for i in c_sup:\n",
    "        for j in communities_flipped[i]:\n",
    "            g_copy.remove_node(j) # May be nothing removed if all communities have overlaps in them\n",
    "\n",
    "    # Build the dictionary of the overlapping intra \n",
    "    for i in g_copy: #g_copy not g\n",
    "        dict_graph_intra_o[i] =[]\n",
    "        for j in g[i]:\n",
    "            k=isbelong(dict_node_communities_flipped[i],dict_node_communities_flipped[j])\n",
    "            if k==1:\n",
    "                dict_graph_intra_o[i].append(j)\n",
    "\n",
    "    # Construct the overlapping intra graph\n",
    "    graph_intra_o = nx.Graph ()\n",
    "    for i in dict_graph_intra_o:\n",
    "        graph_intra_o.add_node(i)\n",
    "\n",
    "    for i in dict_graph_intra_o:\n",
    "        for j in dict_graph_intra_o[i]:\n",
    "            graph_intra_o.add_edges_from ([(i, j)])\n",
    "            \n",
    "    return dict_graph_intra_o, graph_intra_o\n",
    "\n",
    "def inter_wo_o(g,communities_flipped):\n",
    "\n",
    "    dict_node_communities_flipped = dict ()\n",
    "    for i in g: # Get all the nodes and the communities they participate in \n",
    "        dict_node_communities_flipped[i] = []\n",
    "        for j in communities_flipped:\n",
    "            for k in communities_flipped[j]:\n",
    "                if i == k:\n",
    "                    dict_node_communities_flipped[i].append(j)\n",
    "\n",
    "    dict_graph_inter=dict() # dict of overlapping and non-overlapping\n",
    "    for i in  g:\n",
    "        dict_graph_inter[i]=[]\n",
    "        for j in g[i]:\n",
    "            k=isbelong(dict_node_communities_flipped[i],dict_node_communities_flipped[j])\n",
    "            if k==0:\n",
    "                dict_graph_inter[i].append(j)\n",
    "\n",
    "    graph_inter = nx.Graph() # Graph of inter overlapping + non-overlapping\n",
    "    for i in dict_graph_inter:\n",
    "        graph_inter.add_node (i)\n",
    "\n",
    "    for i in dict_graph_inter:\n",
    "        for j in dict_graph_inter[i]:\n",
    "            graph_inter.add_edges_from ([(i, j)])\n",
    "    return dict_graph_inter, graph_inter\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "G.remove_edges_from(nx.selfloop_edges(G))\n",
    "partition, dict_communities_organized =dict_communities_organized_G(G)\n",
    "communities=partition\n",
    "g=G\n",
    "\n",
    "lo, membership_all_nodes, membership_overlapping_nodes = list_overlapping(g, dict_communities_organized)\n",
    "\n",
    "#dict_graph_wo contain dic={nodeid:[node ids in same/different community]} \n",
    "g_wo, dict_graph_wo, partition_wo = remove_overlapping(g, lo, dict_communities_organized)\n",
    "\n",
    "#g_intra_wo contain dic={nodeid:[node ids in same community]} #All nodes and inner community link\n",
    "#graph_wo_intra store the connectivity in graph format\n",
    "g_intra_wo, graph_wo_intra = intra_wo(dict_graph_wo, dict_communities_organized) \n",
    "\n",
    "#g_intra_wo contain dic={nodeid:[node ids in different community]} #All nodes and outer community link\n",
    "#g_inter store the connectivity in graph format\n",
    "g_inter, graph_inter = inter_wo_o(g, dict_communities_organized)\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "#For getting original graph\n",
    "dict_graph = dict()\n",
    "for i in g:\n",
    "    dict_graph[i] = []\n",
    "    for j in g[i]:\n",
    "        dict_graph[i].append(j)\n",
    "\n",
    "# print('Raw graph information:')\n",
    "# print(nx.info(g))\n",
    "\n",
    "# Convert the different dictionaries into graphs to be for calculation of different centralities\n",
    "g_intra_wo_converted = nx.Graph(g_intra_wo)  #All nodes and inner community link\n",
    "g_inter_converted = nx.Graph(g_inter) #All nodes and outer community link\n",
    "dict_graph_converted = nx.Graph(dict_graph) ##All nodes and all inner/outer community link\n",
    "##############################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18085972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1638ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CVoteRAnk: Measure Community based spreading ability\n",
    "\n",
    "#Global ranking: Done by Shannon Entropy\n",
    "GCdict={}\n",
    "GCdicts={}\n",
    "CDR={}\n",
    "CDRs={}\n",
    "import math\n",
    "for i in G.nodes():        #for each node \n",
    "    CD=0\n",
    "    OC=0\n",
    "    com=set(communities.values()) \n",
    "    for j in com:             #for each community\n",
    "        nei=list(g.neighbors(i))\n",
    "        count=0\n",
    "        prob=1\n",
    "        for k in nei:\n",
    "            if communities[k]==j:\n",
    "                count=count+1\n",
    "                \n",
    "        #print(count,len(nei))\n",
    "        \n",
    "        prob=count/len(nei)\n",
    "        if prob!=0:\n",
    "            CD=CD+(-prob*(math.log(prob,10)))\n",
    "    CDR[i]=CD\n",
    "GCdict=CDR\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "#local: by local betweenness\n",
    "LCdict = nx.betweenness_centrality(g_intra_wo_converted, normalized = True, endpoints = False) \n",
    "##############################################################################################################\n",
    "#Compute the final ranking by global, local and community ranking\n",
    "CBFdict={}    \n",
    "for item in G.nodes():\n",
    "    CBFdict[item]=math.sqrt((1+GCdict[item])*(1+LCdict[item]))\n",
    "##############################################################################################################\n",
    "############################################ save in pickle format ###########################################\n",
    "import pickle\n",
    "try:  \n",
    "    filep = open('CBFdict_p', 'wb')\n",
    "    pickle.dump(CBFdict, filep)\n",
    "    filep.close()\n",
    "    \n",
    "except:\n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100660a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a40c45cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1,\n",
       " 2: 1,\n",
       " 3: 1,\n",
       " 4: 1,\n",
       " 5: 1,\n",
       " 6: 1,\n",
       " 7: 0,\n",
       " 8: 0,\n",
       " 9: 0,\n",
       " 10: 0,\n",
       " 11: 0,\n",
       " 12: 2,\n",
       " 13: 2,\n",
       " 14: 2,\n",
       " 15: 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To know the community partition\n",
    "partition  \n",
    "\n",
    "#We get {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 2, 13: 2, 14: 2, 15: 2} for the Toy network\n",
    "#implies: nodes in one community are:{1, 2, 3, 4, 5, 6}, \n",
    "#nodes in another community are {7, 8, 9, 10, 11}, \n",
    "#nodes in another community are {12, 13, 14, 15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b3f5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7cf5cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.25\n",
      "CVoteRank done\n",
      "Saved all pickale successfully\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "import pickle\n",
    "import math\n",
    "###########################################################################################################\n",
    "def secondHN(G,node):\n",
    "    neighbor1=[n for n in G.neighbors(node)]\n",
    "    #print(node1, \"1st neighbors are\", neighbor1)\n",
    "    n2=[]\n",
    "    for node2 in neighbor1:     \n",
    "        neighbor2=[m for m in G.neighbors(node2)]\n",
    "        n2=n2+neighbor2\n",
    "    #for getting the unique node from list\n",
    "    node2 = []\n",
    "    for x in n2:\n",
    "        if x not in node2:\n",
    "            if x not in neighbor1:\n",
    "                node2.append(x) \n",
    "            \n",
    "    node2.remove(node)\n",
    "    return node2\n",
    "\n",
    "###########################################################################################################\n",
    "def CBVote(G, n):   #Proposed\n",
    "    capability = {}\n",
    "    Nodes = list(G.nodes())\n",
    "    totdeg = 0\n",
    "    for node in Nodes:\n",
    "        capability[node] = [0, CBFdict[node]]\n",
    "        totdeg += G.degree(node)\n",
    "    totdeg /= len(Nodes)\n",
    "    val = 1 / totdeg\n",
    "    pbestnode=-1\n",
    "    selected = []\n",
    "    for i in range(n):\n",
    "        for node in Nodes:\n",
    "            capability[node][0] = 0\n",
    "            #print(\"Updated voting ability of of node\",i,\"is\", capability[node][1]) #initialy it is CBFdict, then updated in each iteraction\n",
    "        bes = 0\n",
    "        bestnode = -1\n",
    "        for node in Nodes:\n",
    "            allnodes = list(G.neighbors(node))\n",
    "            if pbestnode!=-1: #next iteration, previously selcted node will not perticipate\n",
    "                if pbestnode in allnodes:\n",
    "                    allnodes.remove(pbestnode)\n",
    "                    \n",
    "            for neig in allnodes:\n",
    "                if capability[neig][1] > 0:\n",
    "                    capability[node][0] += capability[neig][1]   \n",
    "            capability[node][0] = capability[node][1]*(CBFdict[node]+math.sqrt(capability[node][0]))   #\n",
    "            #print(\"Computed voting Score of\", node, \"is\", capability[node][0])\n",
    "            if capability[node][0] > bes and node not in selected:\n",
    "                bes = capability[node][0]\n",
    "                bestnode = node\n",
    "        if bestnode == -1:\n",
    "            break\n",
    "        pbestnode=bestnode\n",
    "        selected.append(bestnode)\n",
    "        capability[bestnode][1]=0\n",
    "        #print(\"Best node is\",selected)\n",
    "        neighbors = list(G.neighbors(bestnode))\n",
    "        for node in neighbors:\n",
    "            capability[node][1] = 0.1*capability[node][1]\n",
    "            #print(\"updated capability of \",node,\"is\",capability[node][1])\n",
    "        node2=secondHN(G,bestnode)\n",
    "        \n",
    "        for neig in node2:\n",
    "            capability[neig][1] = math.sqrt(0.1)*capability[neig][1]\n",
    "            #print(\"updated capability of \",neig,\"is\",capability[neig][1])\n",
    "\n",
    "        caps = {}\n",
    "        for i in G.nodes():\n",
    "            caps[i] = capability[i][0]\n",
    "    return selected, caps\n",
    "\n",
    "###########################################################################################################\n",
    "def SIRModel(g, spreaders, beta, S):\n",
    "    infected_scale = np.array(np.zeros(50))\n",
    "    ftc = 0\n",
    "    while S > 0:\n",
    "        S = S - 1\n",
    "        infected = spreaders\n",
    "        status = {}\n",
    "        for i in g.nodes():\n",
    "            status[i] = 0\n",
    "        for i in infected:\n",
    "            status[i] = 1\n",
    "        n = g.number_of_nodes()\n",
    "        infected_nodes = len(infected)\n",
    "        recovered_nodes = 0\n",
    "        time_stamp = 0\n",
    "        infected_scale[time_stamp] = infected_scale[time_stamp] + (infected_nodes + recovered_nodes) / n\n",
    "        infected = spreaders\n",
    "        while len(infected) > 0:\n",
    "            susceptible_to_infected = []\n",
    "            time_stamp = time_stamp + 1\n",
    "            for i in infected:\n",
    "                susceptible = []\n",
    "                status[i] = 2\n",
    "                for neighbor in g.neighbors(i):\n",
    "                    if status[neighbor] == 0:\n",
    "                        susceptible.append(neighbor)\n",
    "                total_susceptible = len(susceptible)\n",
    "                no_of_susceptible_to_infected = round(beta * total_susceptible)\n",
    "                while no_of_susceptible_to_infected > 0:\n",
    "                    random_index = random.randint(0, total_susceptible - 1)\n",
    "                    if susceptible[random_index] not in susceptible_to_infected:\n",
    "                        susceptible_to_infected.append(susceptible[random_index])\n",
    "                        status[susceptible[random_index]] = 1\n",
    "                        no_of_susceptible_to_infected = no_of_susceptible_to_infected - 1\n",
    "            infected_nodes = len(susceptible_to_infected)\n",
    "            recovered_nodes = len(infected)\n",
    "            ftc = ftc + recovered_nodes / n\n",
    "            infected_scale[time_stamp] = infected_scale[time_stamp] + (infected_nodes + recovered_nodes) / n\n",
    "            infected = susceptible_to_infected\n",
    "    return infected_scale, ftc\n",
    "\n",
    "#####################################################################################################################\n",
    "def Find_Spreaders_Updated(g, shorted_list, no_of_spreaders):\n",
    "    return shorted_list[:no_of_spreaders]\n",
    "\n",
    "\n",
    "def dict2list(d, k):\n",
    "    selected = []\n",
    "    i = 0\n",
    "    for node in sorted(d.items(), key=itemgetter(1), reverse=True):\n",
    "        if i < k:\n",
    "            selected.append(node[0])\n",
    "            i += 1\n",
    "    return selected\n",
    "\n",
    "def dict2list(d, k):\n",
    "    selected = []\n",
    "    i = 0\n",
    "    for node in sorted(d.items(), key=itemgetter(1), reverse=True):\n",
    "        if i < k:\n",
    "            selected.append(node[0])\n",
    "            i += 1\n",
    "    return selected\n",
    "                             \n",
    "######################################################################################################################\n",
    "nodes = list(G.nodes())\n",
    "edges = list(G.edges())\n",
    "\n",
    "edges = list(G.edges())\n",
    "nodes = list(G.nodes())\n",
    "\n",
    "print(0.15*(g.number_of_nodes()))\n",
    "\n",
    "k= int(0.15*(g.number_of_nodes()))\n",
    "k=5 ##for toy network, we want top 5 spreaders \n",
    "S=100\n",
    "T=50\n",
    "beta=0.01\n",
    "#print(k)\n",
    "\n",
    "\n",
    "CVoterank_selected, CV_capsP = CBVote(G, k)\n",
    "print(\"CVoteRank done\")\n",
    "\n",
    "                             \n",
    "######################################################################################################################\n",
    "\n",
    "CV_capsP_sel = CVoterank_selected\n",
    "                \n",
    "######################################################################################################################\n",
    "import pickle\n",
    "try:  \n",
    "   \n",
    "    filep = open('CVoterank_selected', 'wb')\n",
    "    pickle.dump(CVoterank_selected, filep)\n",
    "    filep.close()\n",
    "    \n",
    "    print(\"Saved all pickale successfully\")\n",
    "    \n",
    "except:\n",
    "    print(\"Something went wrong\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0f861cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 15, 3, 9, 8]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CVoterank_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093cf743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d497c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import queue\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "\n",
    "\n",
    "def SIRModel(g, spreaders, beta, S):\n",
    "    infected_scale = np.array(np.zeros(T))\n",
    "    ftc = np.array(np.zeros(T))\n",
    "    while S > 0:\n",
    "        S = S - 1\n",
    "        infected = spreaders\n",
    "        status = {}\n",
    "        for i in g.nodes():\n",
    "            status[i] = 0\n",
    "        for i in infected:\n",
    "            status[i] = 1\n",
    "        n = g.number_of_nodes()\n",
    "        infected_nodes = len(infected)\n",
    "        recovered_nodes = 0\n",
    "        time_stamp = 0\n",
    "        infected_scale[time_stamp] = infected_scale[time_stamp] + (infected_nodes + recovered_nodes) / n\n",
    "        infected = spreaders\n",
    "        while len(infected) > 0:\n",
    "            susceptible_to_infected = []\n",
    "            time_stamp = time_stamp + 1\n",
    "            for i in infected:\n",
    "                susceptible = []\n",
    "                status[i] = 2\n",
    "                for neighbor in g.neighbors(i):\n",
    "                    if status[neighbor] == 0:\n",
    "                        susceptible.append(neighbor)\n",
    "                total_susceptible = len(susceptible)\n",
    "                no_of_susceptible_to_infected = round(beta * total_susceptible)\n",
    "                while no_of_susceptible_to_infected > 0:\n",
    "                    random_index = random.randint(0, total_susceptible - 1)\n",
    "                    if susceptible[random_index] not in susceptible_to_infected:\n",
    "                        susceptible_to_infected.append(susceptible[random_index])\n",
    "                        status[susceptible[random_index]] = 1\n",
    "                        no_of_susceptible_to_infected = no_of_susceptible_to_infected - 1\n",
    "            infected_nodes = len(susceptible_to_infected)\n",
    "            recovered_nodes = len(infected)\n",
    "            ftc[time_stamp] = ftc[time_stamp] + recovered_nodes / n\n",
    "            infected_scale[time_stamp] = infected_scale[time_stamp] + (infected_nodes + recovered_nodes) / n\n",
    "            infected = susceptible_to_infected\n",
    "    return infected_scale, ftc\n",
    "\n",
    "\n",
    "\n",
    "def ftToftc(ft):\n",
    "    ftc = [0 for i in range(0, T + 1)]\n",
    "    for i in range(T):\n",
    "        ftc[i + 1] = ft[i] + ftc[i]\n",
    "    return ftc\n",
    "\n",
    "\n",
    "def SPL(G, spreaders):\n",
    "    total = 0\n",
    "    n = len(spreaders)\n",
    "    c = 0\n",
    "    for i in range(0, n):\n",
    "        node_i = spreaders[i]\n",
    "        for j in range(i + 1, n):\n",
    "            node_j = spreaders[j]\n",
    "            if nx.has_path(G, spreaders[i], spreaders[j]):\n",
    "                c = c + 1\n",
    "\n",
    "                total = total + nx.shortest_path_length(G, node_i, node_j, 'weight')\n",
    "    # print(total)\n",
    "    # print(n)\n",
    "    total = total / (n * n - n)\n",
    "    total = 2 * total\n",
    "    return total\n",
    "\n",
    "\n",
    "def SIR(G, selected, beta, ft, ftc):\n",
    "    Q = queue.Queue(maxsize=len(list(G.nodes())))\n",
    "    recovered = []\n",
    "    infected = []\n",
    "    for node in selected:\n",
    "        Q.put(node)\n",
    "        infected.append(node)\n",
    "    t = 0\n",
    "    while t < T:\n",
    "        tot = []\n",
    "        while not Q.empty():\n",
    "            curr = Q.get()\n",
    "            if curr in recovered:\n",
    "                continue\n",
    "            recovered.append(curr)\n",
    "            ne = list(G.neighbors(curr))\n",
    "            random.shuffle(ne)\n",
    "            neighbors = int(len(ne) * beta) + 1\n",
    "            count = 0\n",
    "            idx = 0\n",
    "            while count < neighbors and idx < len(ne):\n",
    "                if ne[idx] not in recovered and ne[idx] not in tot:\n",
    "                    tot.append(ne[idx])\n",
    "                    count += 1\n",
    "                idx += 1\n",
    "        for i in tot:\n",
    "            Q.put(i)\n",
    "        tot.clear()\n",
    "        ftc[t] = ftc[t]+ len(recovered)\n",
    "        ft[t] = ft[t]+len(recovered)+Q.qsize()\n",
    "        t += 1\n",
    "    return ft, ftc\n",
    "\n",
    "\n",
    "def SIRcall(G, selected, beta, S):\n",
    "    ft = [0.0 for i in range(T)]\n",
    "    ftc = [0.0 for i in range(T)]\n",
    "    for i in range(S):\n",
    "        ft, ftc = SIR(G, selected, beta, ft, ftc)\n",
    "    ft[:] = [x/S for x in ft]\n",
    "    ftc[:] = [x/S for x in ftc]\n",
    "    return ft, ftc\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n",
    "##############################################\n",
    "#for F(t) and F(tc) vs T experiment\n",
    "k= int(0.05*(g.number_of_nodes()))\n",
    "S=1000\n",
    "T=50\n",
    "beta=0.01\n",
    "#print(k)\n",
    "    \n",
    "#From the ranking link, we select 0.05% spreaders \n",
    "CV_capsP_sel_p = CVoterank_selected[0:k]\n",
    "\n",
    "\n",
    "\n",
    "ft_CV_capsP, ftc_CV_capsP = SIRcall(G, CV_capsP_sel_p, beta, S)\n",
    "print(\"CV_capsP SIR Done\")\n",
    "\n",
    "\n",
    "######################################################################################################################\n",
    "import pickle\n",
    "try:  \n",
    "   \n",
    "    filep = open('CVoteRank_ft', 'wb')\n",
    "    pickle.dump(ft_CV_capsP, filep)\n",
    "    filep.close()\n",
    "    \n",
    "    filep = open('CVoteRank_ftc', 'wb')\n",
    "    pickle.dump(ftc_CV_capsP, filep)\n",
    "    filep.close()\n",
    "\n",
    "    \n",
    "    print(\"Saved all pickale successfully\")\n",
    "    \n",
    "except:\n",
    "    print(\"Something went wrong\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f9617b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ce3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b850a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftc10 = [] #\n",
    "\n",
    "# S = int(input(\"Input the number of times SIR should average out: \"))\n",
    "# beta = float(input(\"Input another beta: \"))\n",
    "S = 1000\n",
    "beta = 0.01\n",
    "\n",
    "\n",
    "\n",
    "spreaders_fractions_list = [0.03, 0.06, 0.09, 0.12, 0.15]\n",
    "for fraction in spreaders_fractions_list:\n",
    "    #print(fraction)\n",
    "    spreaders = int(len(G) * fraction)\n",
    "    #print(spreaders)\n",
    "    \n",
    "    # Proposed\n",
    "    spreaders_list_CV_capsP = Find_Spreaders_Updated(G, CV_capsP_sel, spreaders)\n",
    "    ft, ftc = SIRcall(G, spreaders_list_CV_capsP, beta, S)\n",
    "    ftc10.append(ftc[len(ftc)-1])\n",
    "    \n",
    "######################################################################################################################\n",
    "try:  \n",
    "   \n",
    "    filep = open('CVoteRank_fractions_list', 'wb')\n",
    "    pickle.dump(ftc10, filep)\n",
    "    filep.close()\n",
    "\n",
    "    \n",
    "    print(\"Saved all pickale successfully\")\n",
    "    \n",
    "except:\n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d65cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftc10 = [] #\n",
    "\n",
    "S = 1000\n",
    "\n",
    "beta_list = [0.01, 0.05, 0.10, 0.15]\n",
    "for fraction in beta_list:\n",
    "    print(fraction)\n",
    "    spreaders=int(0.05*(g.number_of_nodes()))\n",
    "    beta = fraction\n",
    "\n",
    "    # Proposed\n",
    "    spreaders_list_CV_capsP = Find_Spreaders_Updated(G, CV_capsP_sel_p, spreaders)\n",
    "    ft, ftc = SIRcall(G, spreaders_list_CV_capsP, beta, S)\n",
    "    ftc10.append(ftc[len(ftc)-1])\n",
    "\n",
    "######################################################################################################################\n",
    "try:  \n",
    "   \n",
    "    filep = open('CVoteRank_beta', 'wb')\n",
    "    pickle.dump(ftc10, filep)\n",
    "    filep.close()\n",
    "\n",
    "    \n",
    "    print(\"Saved all pickale successfully\")\n",
    "    \n",
    "except:\n",
    "    print(\"Something went wrong\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a91fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135623a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ls(g, infeacted_set):\n",
    "    \"\"\"compute the average shortest path in the initial node set\n",
    "     # Arguments\n",
    "         g: a graph as networkx Graph\n",
    "         infeacted_set: the initial node set\n",
    "     Returns\n",
    "         return the average shortest path\n",
    "     \"\"\"\n",
    "    dis_sum = 0\n",
    "    path_num = 0\n",
    "    for u in infeacted_set:\n",
    "        for v in infeacted_set:\n",
    "            if u != v:\n",
    "                try:\n",
    "                    dis_sum += nx.shortest_path_length(g, u, v)\n",
    "                    path_num += 1\n",
    "                except:\n",
    "                    dis_sum += 0\n",
    "                    path_num -= 1\n",
    "    return dis_sum / path_num\n",
    "\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "#r = [0.05, 0.1, 0.15, 0.2, 0.25]\n",
    "r=[0.03,0.06,0.09,0.12,0.15]\n",
    "\n",
    "topk_list = []\n",
    "for k in r:\n",
    "    topk = round(nx.number_of_nodes(G) * k)\n",
    "    print(k, topk)\n",
    "    topk_list.append(topk)\n",
    "    \n",
    "max_ = r[-1]\n",
    "max_topk = round(max_ * nx.number_of_nodes(G))\n",
    "print(max_topk)\n",
    "\n",
    "  \n",
    "CV_capsPRank = CV_capsP_sel\n",
    "\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "CV_capsP_ls = []\n",
    "#######################################################################################################    \n",
    "for k in tqdm(topk_list):\n",
    "    topk = k       \n",
    "    CV_capsP_ls.append(get_ls(G, [x for x in CV_capsPRank[:topk]]))\n",
    "    \n",
    "####################################################################################################### \n",
    "\n",
    "try:  \n",
    "   \n",
    "    filep = open('CVoteRank_distance', 'wb')\n",
    "    pickle.dump(CV_capsP_ls, filep)\n",
    "    filep.close()\n",
    "\n",
    "    \n",
    "    print(\"Saved all pickale successfully\")\n",
    "    \n",
    "except:\n",
    "    print(\"Something went wrong\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a358c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
